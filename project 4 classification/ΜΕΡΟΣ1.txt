

---

### 1 Εφαρμογή σε απλό dataset

Στην πρώτη φάση της εργασίας, επιλέγεται από το UCI repository το **Haberman's Survival**, το οποίο περιλαμβάνει 306 δείγματα (instances), από 3 χαρακτηριστικά (attributes) το καθένα. Ακολουθούνται τα εξής βήματα:

• **Διαχωρισμός σε σύνολα εκπαίδευσης-επικύρωσης-ελέγχου**: Σε πρώτη φάση είναι απαραίτητος ο διαχωρισμός του συνόλου δεδομένων σε τρία μη επικαλυπτόμενα υποσύνολα **Dtrn, Dval, Dchk**, από τα οποία το πρώτο θα χρησιμοποιηθεί για εκπαίδευση, το δεύτερο για επικύρωση και αποφυγή του φαινομένου υπερεκπαίδευσης και το τελευταίο για τον έλεγχο της απόδοσης του τελικού μας μοντέλου.
Προτείνεται να χρησιμοποιηθεί το **60%** του συνόλου των δειγμάτων για το υποσύνολο εκπαίδευσης και από **20%** του συνόλου των δειγμάτων για κάθε ένα από τα δύο εναπομείναντα υποσύνολα. Ένα σημείο στο οποίο θα πρέπει να δοθεί προσοχή είναι το ότι για να επιτύχουμε καλή απόδοση, θα πρέπει η συχνότητα εμφάνισης δειγμάτων που ανήκουν σε μια συγκεκριμένη κλάση, σε κάθε ένα από τα τρία σύνολα διαμέρισης, να είναι όσο το δυνατόν πιο “όμοια” με την αντίστοιχη συχνότητα εμφάνισής τους στο αρχικό σύνολο δεδομένων.

• **Εκπαίδευση TSK μοντέλων με διαφορετικές παραμέτρους**: Σε αυτό το στάδιο θα εξεταστούν διάφορα μοντέλα TSK όσον αφορά την απόδοσή τους στο σύνολο ελέγχου. Συγκεκριμένα, θα εκπαιδευτούν **τέσσερα TSK μοντέλα**, στα οποία θα μεταβάλλεται το πλήθος των ασαφών IF-THEN κανόνων.

Σκοπός είναι να μελετηθεί η επίδραση της διαμέρισης του χώρου εισόδου – σε συνάρτηση με την πολυπλοκότητα που αυτή επιφέρει, στην απόδοση του ταξινομητή. Η διαμέριση του χώρου εισόδου θα γίνει με τη μέθοδο του **Subtractive Clustering (SC)** και τα TSK μοντέλα που θα προκύψουν θα διαφέρουν ως προς την παράμετρο που καθορίζει τον αριθμό των κανόνων.

* Στην πρώτη περίπτωση (δύο πρώτα μοντέλα), το subtractive clustering θα εκτελεστεί για όλα τα δεδομένα του συνόλου εκπαίδευσης (**class independent**).
* Στη δεύτερη (επόμενα δύο), θα εξεταστεί ο διαμερισμός του χώρου εισόδου εφαρμόζοντας clustering στα δεδομένα του συνόλου εκπαίδευσης που ανήκουν στην εκάστοτε κλάση ξεχωριστά (**class dependent**).

Ο λόγος για τον οποίο γίνεται αυτό είναι η αύξηση της ερμηνευσιμότητας του μοντέλου και η παραγωγή “καθαρότερων” clusters (άρα και κανόνων).

**Σημείωση:** Εφόσον η έξοδός μας αποτελείται από έναν ακέραιο αριθμό, ενδεικτικό της κλάσης στην οποία ανήκει το εκάστοτε δείγμα, προτείνεται η χειροκίνητη αλλαγή του τύπου συνάρτησης εξόδου από γραμμική σε **singleton**. Για τις δύο περιπτώσεις που θα εξεταστούν (class independent - class dependent) η παράμετρος που καθορίζει το μέγεθος των clusters (και τον αριθμό των κανόνων) να λάβει δύο ακραίες τιμές, έτσι ώστε ο αριθμός των κανόνων στα δύο μοντέλα που θα προκύψουν να παρουσιάζει σημαντική διαφορά.

Και τα τέσσερα μοντέλα να εκπαιδευτούν με την **υβριδική μέθοδο**, σύμφωνα με την οποία:

* οι παράμετροι των συναρτήσεων συμμετοχής βελτιστοποιούνται μέσω της μεθόδου της **οπισθοδιάδοσης (backpropagation algorithm)**
* οι παράμετροι της συνάρτησης εξόδου βελτιστοποιούνται μέσω της μεθόδου των **ελαχίστων τετραγώνων (Least Squares)**.

---

#### Αξιολόγηση μοντέλων

Για την αξιολόγηση της ταξινόμησης των δειγμάτων από τα διάφορα μοντέλα, θα χρησιμοποιηθούν οι εξής δείκτες απόδοσης:

1. **Error matrix**: Ο πίνακας σφαλμάτων ταξινόμησης είναι ένας k×k πίνακας, με k τον αριθμό των κλάσεων. Η γενική του δομή παρουσιάζεται στον πίνακα 1.

   * Τα στοιχεία της κύριας διαγωνίου περιλαμβάνουν το πλήθος των δειγμάτων που ανήκουν σε μια συγκεκριμένη κλάση και τα οποία ορθώς ταξινομήθηκαν.
   * Τα στοιχεία εκτός διαγωνίου περιλαμβάνουν τα λανθασμένα ταξινομημένα δείγματα.

2. **Overall accuracy (OA):** Η συνολική ακρίβεια ορίζεται ως το ποσοστό των ορθώς ταξινομημένων δειγμάτων ως προς το συνολικό πλήθος των δειγμάτων.
   [
   OA = \frac{1}{N} \sum_{i=1}^{k} x_{ii}
   ]

3. **Producer's accuracy (PA) & User's accuracy (UA):**

   * ( PA(j) = \frac{x_{jj}}{x_{jc}} )
   * ( UA(i) = \frac{x_{ii}}{x_{ir}} )

4. **Στατιστικό μέγεθος κ̂:**
   [
   κ̂ = \frac{N\sum_{i=1}^{k} x_{ii} - \sum_{i=1}^{k} x_{ir}x_{ic}}{N^2 - \sum_{i=1}^{k} x_{ic}x_{ir}}
   ]

---

#### Ζητούμενα του προβλήματος

Για κάθε ένα από τα τέσσερα TSK μοντέλα:

1. Να δοθούν τα διαγράμματα με τις τελικές μορφές των ασαφών συνόλων που προέκυψαν.
2. Να δοθούν τα **learning curves**, δηλαδή το σφάλμα του μοντέλου ως προς τις επαναλήψεις.
3. Να παρουσιαστεί ο πίνακας σφαλμάτων ταξινόμησης και να εξαχθούν οι δείκτες **OA, PA, UA, κ̂**.
4. Να σχολιαστούν τα αποτελέσματα:

   * Ποια η επίδραση του αριθμού κανόνων στην απόδοση του ταξινομητή;
   * Τι δείχνει η επικάλυψη των ασαφών συνόλων στις εισόδους για την ενεργοποίηση κανόνων;
   * Προτείνετε πιθανή μέθοδο βελτίωσης της σχεδίασης του τμήματος υπόθεσης.

---

#### Σημείωση για την έξοδο των μοντέλων

Η υλοποίηση των TSK στο MATLAB δίνει **πραγματική έξοδο**, ενώ στα προβλήματα ταξινόμησης η μεταβλητή-στόχος είναι κατηγορική (ακέραιοι).
Δύο λύσεις:

* Στρογγυλοποίηση στον πλησιέστερο ακέραιο.
* Καθορισμός δικού σας σχήματος διακριτοποίησης της συνεχούς εξόδου.

---
