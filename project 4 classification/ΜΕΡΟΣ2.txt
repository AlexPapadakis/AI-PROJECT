Στη δεύτερη φάση της εργασίας θα ακολουθηθεί μια πιο συστηματική προσέγγιση στο πρόβλημα της χρήσης ασαφών νευρωνικών μοντέλων σε προβλήματα
ταξινόμησης. Για το σκοπό αυτό θα επιλεχθεί ένα dataset με υψηλότερο βαθμό
διαστασιμότητας. ΄Ενα προφανές πρόβλημα που ανακύπτει από την επιλογή αυτή,
είναι η λεγόμενη “έκρηξη” του πλήθους των IF-THEN κανόνων (rule explosion).
΄Οπως είναι γνωστό από τη θεωρία, για την κλασική περίπτωση του γριδ παρτιτιονινγ του χώρου εισόδου, ο αριθμός των κανόνων αυξάνεται εκθετικά σε σχέση με
το πλήθος των εισόδων, γεγονός που καθιστά πολύ δύσκολη την μοντελοποίηση
μέσω ενός TSK μοντέλου ακόμα και για datasets μεσαίας κλίμακας.
Το dataset που θα επιλεγεί για την επίδειξη των παραπάνω μεθόδων είναι το
Epileptic Seizure Recognition dataset από το UCI repository. Το συγκεκριμένο
dataset, περιλαμβάνει 11500 δείγματα, καθένα από τα οποία περιγράφεται από 179
μεταβλητές/χαρακτηριστικά. Είναι φανερό ότι το μέγεθος του dataset καθιστά δυσκολότερη μια απλή εφαρμογή ενός TSK μοντέλου, σαν αυτή του προηγούμενου
μέρους της εργασίας. Ο μεγάλος αριθμός μεταβλητών καθιστά αναγκαία τη χρήση
μεθόδων μείωσης της διαστασιμότητας καθώς και του αριθμού των IF-THEN κανόνων. Ο στόχος αυτός θα επιτευχθεί μέσω της επιλογής χαρακτηριστικών και
της χρήσης ασαφούς ομαδοποίησης. Οι δύο αυτές μέθοδοι όμως, παρά τη ελάττωση της πολυπλοκότητας που επιφέρουν, εισάγουν στο πρόβλημα δύο ελεύθερες
παραμέτρους, συγκεκριμένα, τον αριθμό των χαρακτηριστικών προς επιλογή και
τον αριθμό των ομάδων που θα δημιουργηθούν. Η επιλογή των δύο αυτών παραμέτρων επαφίεται στον εκάστοτε χρήστη και είναι ουσιαστική όσον αφορά την
τελική απόδοση του μοντέλου. Στην παρούσα εργασία, θα υλοποιηθεί η μέθοδος αναζήτησης πλέγματος για την εύρεση των βέλτιστων τιμών των παραμέτρων.
Αναλυτικά, η μοντελοποίηση του προβλήματος θα ακολουθήσει τα εξής βήματα:
4
1. Διαχωρισμός σε σύνολα εκπαίδευσης- επικύρωσης – ελέγχου: ΄Οπως και στο
πρώτο κομμάτι της εργασίας, είναι απαραίτητος ο διαχωρισμός του συνόλου
δεδομένων σε τρία υποσύνολα Dtrn, Dval, Dchk, το ένα από τα οποία θα
χρησιμοποιηθεί για εκπαίδευση και το δεύτερο για έλεγχο της απόδοσης.
2. Επιλογή των βέλτιστων παραμέτρων: ΄Οπως αναφέρθηκε παραπάνω, το σύστημά μας περιλαμβάνει δύο ελεύθερες παραμέτρους την τιμή των οποίων πρέπει
να επιλέξουμε εμείς. Η δημοφιλέστερη μέθοδος μέσω της οποίας επιτυγχάνεται αυτό είναι η αναζήτηση πλέγματος. Συγκεκριμένα, αφού λάβουμε
ένα σύνολο τιμών για κάθε παράμετρο, δημιουργούμε ένα n-διάστατο πλέγμα
(στην περίπτωσή μας n = 2), όπου κάθε σημείο αντιστοιχεί σε μια n-άδα
τιμών για τις εν λόγω παραμέτρους, και σε κάθε σημείο χρησιμοποιούμε μια
μέθοδο αξιολόγησης για ελέγξουμε την ορθότητα των συγκεκριμένων τιμών.
Μια καθιερωμένη επιλογή για την αξιολόγηση αυτή αποτελεί η διασταυρωμένη επικύρωση (cross validation). Σύμφωνα με τη μέθοδο αυτή ,και για
επιλεγμένες τιμές των παραμέτρων, χωρίζουμε το σύνολο εκπαίδευσης σε
δύο υποσύνολα, από τα οποία το ένα θα χρησιμοποιηθεί για την εκπαίδευση
ενός μοντέλου και το δεύτερο για την αξιολόγησή του. Η διαδικασία αυτή
επαναλαμβάνεται – συνήθως πέντε ή δέκα φορές – όπου κάθε φορά χρησιμοποιείται διαφορετικός διαχωριμός του συνόλου εκπαίδευσης, και στο τέλος
λαμβάνουμε τον μέσο όρο του σφάλματος του μοντέλου. Η λογική πίσω από
τις πολλαπλές εκπαιδεύσεις και ελέγχους έγκειται στο ότι με αυτό τον τρόπο,
αποκτούμε μια αρκετά καλή εκτίμηση της απόδοσης του μοντέλου, και έμμεσα των τιμών των παραμέτρων με βάση τις οποίες χτίστηκε το μοντέλο.
΄Οταν η παραπάνω διαδικασία εκτελεστεί για κάθε σημείο του πλέγματος,
λαμβάνουμε ως βέλτιστες τιμές των παραμέτρων, τις τιμές που αντιστοιχούν στο μοντέλο που παρουσίασε το ελάχιστο μέσο σφάλμα. Οι τιμές αυτές
χρησιμοποιούνται για την εκπαίδευση του τελικού μας μοντέλου.
Για τους σκοπούς της εργασίας, ορίζουμε τις εξής παραμέτρους:
• Αριθμός χαρακτηριστικών: Το πλήθος των χαρακτηριστικών που θα
χρησιμοποιηθούν στην εκπαίδευση των μοντέλων.
• Ακτίνα των clusters rα: Η παράμετρος που καθορίζει την ακτίνα επιρροής των clusters και κατ΄ επέκταση το πλήθος των κανόνων που θα
προκύψουν.
Ο καθορισμός των τιμών των παραμέτρων που θα εξεταστούν επιλέγεται
ελεύθερα.
3. Με βάση τις βέλτιστες τιμές των παραμέτρων που επιλέχθηκαν από το προηγούμενο βήμα, εκπαιδεύουμε ένα τελικό TSK μοντέλο και ελέγχουμε την
απόδοσή του στο σύνολο ελέγχου.
Τα παραπάνω βήματα συνοψίζουν πλήρως τη διαδικασία μοντελοποίησης που θα
ακολουθηθεί. Ζητούνται τα εξής:
5
1. Ο διαχωρισμός του συνόλου δεδομένων να γίνει όπως και στο πρώτο κομμάτι,
με τα σύνολα εκπαίδευσης-επικύρωσης-ελέγχου να περιλαμβάνουν αντίστοιχα το 60% - 20% - 20% του συνόλου.
2. Να εκτελεστεί αναζήτηση πλέγματος (grid search) και αξιολόγηση μέσω
5-πτυχης διασταυρωμένης επικύρωσης (5-fold cross validation) για την επιλογή των βέλτιστων τιμών των παραμέτρων. Σε κάθε επανάληψη να αποθηκεύεται το μέσο σφάλμα. Ο διαχωρισμός των δεδομένων να γίνει έτσι ώστε
σε κάθε επανάληψη, το 80% των δεδομένων να χρησιμοποιείται για εκπαίδευση και το υπόλοιπο 20% για επικύρωση (ως είσοδοι στη συνάρτηση ans του
MATLAB). Μια συνάρτηση που μπορεί να βοηθήσει σε αυτό το έργο είναι η
cvpartition. Θα πρέπει κι εδώ να δοθεί προσοχή έτσι ώστε η κατανομή των
κλάσεων να διατηρηθεί και στα δύο υποσύνολα. Ως αλγόριθμος επιλογής
χαρακτηρηστικών μπορεί να επιλεχθεί ένας από τους εξής: (Relief, mRMR,
FMI) και ως μέθοδος διαμέρισης διασκορπισμού ο αλγόριθμος Subtractive
Clustering (SC). Να εφαρμοστεί προεπεξεργασία των δεδομένων αν αυτό
κριθεί απαραίτητο. Μετά το πέρας της διαδικασίας, να σχολιαστούν τα αποτελέσματα όσον αφορά το μέσο σφάλμα σε συνάρτηση με τις τιμές των
παραμέτρων. Να δοθούν διαγράμματα τα οποία να απεικονίζουν την καμπύλη
αυτού του σφάλματος σε σχέση με τον αριθμό των κανόνων και σε σχέση με
τον αριθμό των επιλεχθέντων χαρακτηριστικών. Ποιά συμπεράσματα μπορούν να βγουν·
3. Να εκπαιδευτεί το τελικό TSK μοντέλο με τις βέλτιστες τιμές των παραμέτρων. Να δοθούν τα εξής διαγράμματα:
• Διαγράμματα όπου να αποτυπώνονται οι προβλέψεις του τελικού μοντέλου καθώς και οι πραγματικές τιμές.
• Διαγράμματα εκμάθησης όπου να απεικονίζεται το σφάλμα συναρτήσει
του αριθμού επαναλήψεων.
• Να δοθούν ενδεικτικά μερικά ασαφή σύνολα στην αρχική και τελική
τους μορφή.
• Να δοθεί ο πίνακας σφαλμάτων ταξινόμησης και να εξαχθούν από αυτόν
τιμές των δεικτών απόδοσης OA, P A, UA, κˆ
• Τέλος, να σχολιαστούν τα αποτελέσματα όσον αφορά τα χαρακτηριστικά που επιλέχθηκαν και τον αριθμό IF-THEN κανόνων του ασαφούς
συστήματος συμπερασμού. Να γίνει σύγκριση με τον αντίστοιχο αριθμό
κανόνων αν για το ίδιο πλήθος χαρακτηριστικών, είχαμε επιλέξει grid
partitioning με δύο ή τρία ασαφή σύνολα ανά είσοδο. Ποιά είναι τα συμπεράσματα· Τέλος, να γίνουν αντίστοιχα σχόλια όπως και στο πρώτο
τμήμα, σχετικά με την επικάλυχη των προβολών των ασαφών συνόλων
στο χώρο των μεταβλητών εισόδου και την επίδραση του διαμερισμού
του συνολικού χώρου εισόδου στο ποσοστό των ενεργών κανόνων.
Σημείωση:
6
1. ΄Οπως και στο πρώτο κομμάτι της εργασίας, να γίνει χειροκίνητη αλλαγή του
τύπου της συνάρτησης εξόδου των μοντέλων από linear σε constant.
2. Η διαδικασία του subtractive clustering θα πρέπει να εφαρμοστεί ξεχωριστά για κάθε κλάση (με την ίδια τιμή στην παράμετρο που καθορίζει την
ακτίνα.). Μετά τον αρχικό διαχωρισμό των δεδομένων, τα δεδομένα εκπαίδευσης διαμερίζονται επιπλέον ανάλογα με την κλάση στην οποία ανήκουν
και η συνολική βάση κανόνων προκύπτει ως η ένωση των επιμέρους κανόνων
που προκύπτουν από τις class-specic ομαδοποιήσεις. Το σχετικό παράδειγμα που έχει ανέβει στο e-learning παρέχει μια επίδειξη αυτής της διαδικασίας